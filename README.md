End-to-End NLP Pipeline â€” IMDB Sentiment (Daxwell Demo)

ğŸ“˜ Overview

This project demonstrates a complete end-to-end NLP machine learning pipeline for sentiment analysis using the IMDB movie review dataset.
It includes both classic ML models (TF-IDF â†’ GBT / LogReg / MLP) and a Transformer-based model (DistilBERT fine-tuning).
All components are modular, reproducible, and designed with MLflow tracking for experiment management.

<p align="center">
  <img src="pipeline_overview.png" alt="Pipeline Architecture" width="85%">
</p>

âš™ï¸ Features

Data ingestion from Hugging Face IMDB or CSV

Text preprocessing: HTML cleaning, lowercasing, normalization

Feature engineering: TF-IDF vectorization + DistilBERT tokenization

Model training: XGBoost, Logistic Regression, MLP(SVD), and Transformer

Evaluation: Accuracy, F1-score, and AUC

Experiment tracking via MLflow

Artifact persistence for reproducibility

ğŸ“‚ Repository Structure
.
â”œâ”€â”€ IMDB.ipynb                 # Colab/Notebook walkthrough
â”œâ”€â”€ full_pipeline.py           # Modular CLI script (complete pipeline)
â”œâ”€â”€ pipeline_overview.png      # Architecture diagram (used in README)
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ metrics.csv            # Model comparison report
â”œâ”€â”€ models/                    # Trained model binaries (.pkl)
â”œâ”€â”€ vectorizers/               # TF-IDF, SVD, Scaler
â”œâ”€â”€ transformer/model/         # Fine-tuned DistilBERT model
â””â”€â”€ requirements.txt

ğŸš€ Quick Start
1ï¸âƒ£ Install dependencies
pip install -r requirements.txt

2ï¸âƒ£ Run the classic models
python full_pipeline.py --source imdb --run-gbt --run-logreg

3ï¸âƒ£ Add MLP via SVD
python full_pipeline.py --source imdb --run-mlp-svd --svd-components 256

4ï¸âƒ£ Run Transformer
python full_pipeline.py --source imdb --run-transformer --transformer-samples 4000 --transformer-epochs 1

5ï¸âƒ£Enable MLflow logging
python full_pipeline.py --source imdb --run-gbt --run-logreg --run-transformer --use-mlflow

ğŸ“Š Outputs

reports/metrics.csv â€” consolidated performance report

models/ â€” serialized classic models

vectorizers/ â€” TF-IDF, SVD, Scaler

transformer/model/ â€” fine-tuned DistilBERT

mlruns/ â€” MLflow experiment logs

ğŸ§  Key Learnings

Hybrid NLP workflow: traditional + transformer approaches

Efficient model tracking & reproducibility with MLflow

Modular, scalable design suitable for production deployment
