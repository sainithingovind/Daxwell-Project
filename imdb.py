# -*- coding: utf-8 -*-
"""IMDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hnF1E8wfu9HN3spv1SEtIuk1LWkGQAqE
"""

import torch
torch.cuda.is_available()

!pip -q install pandas numpy scikit-learn xgboost matplotlib seaborn scipy tqdm \
              torch torchtext transformers datasets accelerate

# Create folders
import os, numpy as np, random
for p in ["data","models","reports","notebooks"]:
    os.makedirs(p, exist_ok=True)

SEED = 42
random.seed(SEED); np.random.seed(SEED)

# ---- MLflow setup ----
!pip -q install mlflow
import mlflow


mlflow.set_tracking_uri("file:./mlruns")
mlflow.set_experiment("imdb_sentiment")

print("âœ… MLflow initialized (tracking locally in ./mlruns)")

# ==== Safe MLflow logging helper ====
import numpy as np, mlflow
from numbers import Number

def log_metrics_safe(metrics: dict, prefix: str = ""):
    """
    Safely log metrics to MLflow by removing non-numeric, None, Ellipsis, NaN, or Inf values.
    Prevents 'float() argument must be a string or real number' errors.
    """
    clean = {}
    for k, v in metrics.items():
        key = f"{prefix}{k}"
        if v is Ellipsis or v is None:
            continue
        try:
            val = float(v) if isinstance(v, Number) else float(v)
        except Exception:
            continue
        if np.isnan(val) or np.isinf(val):
            continue
        clean[key] = val
    if clean:
        mlflow.log_metrics(clean)
    else:
        print("âš ï¸ No valid numeric metrics found to log.")

import re, html, pandas as pd, numpy as np
import matplotlib.pyplot as plt; import seaborn as sns
from tqdm import tqdm; tqdm.pandas()

def clean_text(s: str) -> str:
    if not isinstance(s, str): return ""
    s = html.unescape(s)
    s = re.sub(r"<br\s*/?>", " ", s)
    s = re.sub(r"http\S+", " ", s)
    s = s.lower()
    # decontractions
    s = re.sub(r"won't", "will not", s); s = re.sub(r"can't", "can not", s)
    s = re.sub(r"n't", " not", s); s = re.sub(r"'re", " are", s)
    s = re.sub(r"'s", " is", s); s = re.sub(r"'d", " would", s)
    s = re.sub(r"'ll", " will", s); s = re.sub(r"'t", " not", s)
    s = re.sub(r"'ve", " have", s); s = re.sub(r"'m", " am", s)
    s = re.sub(r"[^a-z0-9\s\.!,?']", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

from datasets import load_dataset
ds = load_dataset("imdb")

train_df = pd.DataFrame({"text": ds["train"]["text"], "label": ds["train"]["label"]})
test_df  = pd.DataFrame({"text": ds["test"]["text"],  "label": ds["test"]["label"]})

# Cleaned versions
train_df["text_clean"] = train_df["text"].progress_apply(clean_text)
test_df["text_clean"]  = test_df["text"].progress_apply(clean_text)

# Save to disk
train_df.to_csv("data/imdb_train.csv", index=False)
test_df.to_csv("data/imdb_test.csv", index=False)

len(train_df), len(test_df), train_df.head(2)

train_df.head()

def eda(df, name):
    print(f"=== {name.upper()} ===")
    print("shape:", df.shape)
    print("label balance:", df["label"].value_counts(normalize=True).round(3).to_dict())
    df["len"] = df["text_clean"].str.split().str.len()
    print("length mean/median:", round(df["len"].mean(),1), "/", int(df["len"].median()))
    fig, axes = plt.subplots(1,2, figsize=(12,4))
    sns.countplot(data=df, x="label", ax=axes[0])
    axes[0].set_title(f"{name} labels (0=neg, 1=pos)")
    axes[1].hist(df["len"], bins=50)
    axes[1].set_title(f"{name} review length")
    plt.show()

eda(train_df, "train")
eda(test_df, "test")

from sklearn.feature_extraction.text import TfidfVectorizer
from scipy import sparse
import joblib

vec = TfidfVectorizer(max_features=50000, ngram_range=(1,2), stop_words="english")
Xtr = vec.fit_transform(train_df["text_clean"].tolist())
Xte = vec.transform(test_df["text_clean"].tolist())
ytr = train_df["label"].values
yte = test_df["label"].values

# Persist for downstream steps
joblib.dump(vec, "models/tfidf.joblib")
sparse.save_npz("data/X_train_tfidf.npz", Xtr)
sparse.save_npz("data/X_test_tfidf.npz",  Xte)
np.save("data/y_train.npy", ytr)
np.save("data/y_test.npy",  yte)

Xtr.shape, Xte.shape, ytr.mean(), yte.mean()

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

Xtr = sparse.load_npz("data/X_train_tfidf.npz")
Xte = sparse.load_npz("data/X_test_tfidf.npz")
ytr = np.load("data/y_train.npy"); yte = np.load("data/y_test.npy")

gbt = XGBClassifier(
    n_estimators=600, max_depth=8, learning_rate=0.05,
    subsample=0.9, colsample_bytree=0.8, tree_method="hist", random_state=SEED, n_jobs=-1
)
gbt.fit(Xtr, ytr)
p = gbt.predict(Xte); proba = gbt.predict_proba(Xte)[:,1]
gbt_metrics = {"model":"GBT",
               "acc": float(accuracy_score(yte,p)),
               "f1":  float(f1_score(yte,p)),
               "auc": float(roc_auc_score(yte,proba))}
gbt_metrics

from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(hidden_layer_sizes=(512,128), activation="relu",
                    batch_size=256, max_iter=10, random_state=SEED)
mlp.fit(Xtr, ytr)
p = mlp.predict(Xte); proba = mlp.predict_proba(Xte)[:,1]
mlp_metrics = {"model":"MLP",
               "acc": float(accuracy_score(yte,p)),
               "f1":  float(f1_score(yte,p)),
               "auc": float(roc_auc_score(yte,proba))}
mlp_metrics

# --- Improved RNN (BiLSTM + pack_padded + max-pool) ---
import re, json, math, numpy as np
import torch, torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from collections import Counter

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
torch.manual_seed(42); np.random.seed(42)

# Simple tokenizer aligned with clean_text()
_word_re = re.compile(r"[a-z0-9']+|[.!?,]")
def basic_tokenize(s: str):
    if not isinstance(s, str): return []
    return _word_re.findall(s.lower())

class TextDataset(Dataset):
    def __init__(self, texts, labels, vocab=None, max_len=256, build_vocab=False, vocab_size=20000, min_freq=2):
        self.labels = np.asarray(labels).astype(np.float32)
        self.tokens = [basic_tokenize(t) for t in texts]
        self.max_len = max_len

        if build_vocab:
            cnt = Counter()
            for toks in self.tokens: cnt.update(toks)
            self.stoi = {"<pad>":0, "<unk>":1}
            for w, c in cnt.most_common():
                if c < min_freq: break
                if len(self.stoi) >= vocab_size: break
                self.stoi[w] = len(self.stoi)
        else:
            self.stoi = vocab

    def encode(self, toks):
        ids = [self.stoi.get(w, 1) for w in toks]  # 1 = <unk>
        length = min(len(ids), self.max_len)
        ids = ids[:self.max_len]
        if len(ids) < self.max_len:
            ids = ids + [0] * (self.max_len - len(ids))  # 0 = <pad>
        return ids, length

    def __len__(self): return len(self.labels)
    def __getitem__(self, i):
        ids, length = self.encode(self.tokens[i])
        x = torch.tensor(ids, dtype=torch.long)
        y = torch.tensor(self.labels[i], dtype=torch.float32)
        l = torch.tensor(length, dtype=torch.long)
        return x, l, y

def collate(batch):
    xs, ls, ys = zip(*batch)
    return torch.stack(xs), torch.stack(ls), torch.stack(ys)

class BiLSTMMaxPool(nn.Module):
    def __init__(self, vocab_size, emb=128, hid=128, p_drop=0.2):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, emb, padding_idx=0)
        self.lstm = nn.LSTM(emb, hid, batch_first=True, bidirectional=True)
        self.drop = nn.Dropout(p_drop)
        self.fc = nn.Linear(hid*2, 1)

    def forward(self, x, lengths):
        # x: (B, T), lengths: (B,)
        emb = self.emb(x)  # (B, T, E)
        # pack to ignore pads
        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)
        packed_out, _ = self.lstm(packed)
        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B, T*, 2H)
        # max-pool over time on valid steps only
        mask = (x != 0)[:, :out.size(1)].unsqueeze(-1)  # (B, T*, 1)
        out_masked = out.masked_fill(~mask, float('-inf'))
        pooled, _ = torch.max(out_masked, dim=1)  # (B, 2H)
        pooled = torch.nan_to_num(pooled, nan=0.0, neginf=0.0, posinf=0.0)
        logits = self.fc(self.drop(pooled)).squeeze(1)  # (B,)
        return torch.sigmoid(logits)

# Build datasets
train_texts = train_df["text_clean"].tolist()
test_texts  = test_df["text_clean"].tolist()
y_train = train_df["label"].values
y_test  = test_df["label"].values

train_ds = TextDataset(train_texts, y_train, build_vocab=True, vocab_size=20000, max_len=256, min_freq=2)
vocab = train_ds.stoi
test_ds  = TextDataset(test_texts,  y_test,  vocab=vocab, max_len=256)

tr_dl = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=2, pin_memory=True, collate_fn=collate)
te_dl = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate)

model = BiLSTMMaxPool(vocab_size=len(vocab), emb=128, hid=128, p_drop=0.2).to(DEVICE)
opt = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-5)
lossf = nn.BCELoss()

def run_epoch(dl, train=True, clip=1.0):
    from sklearn.metrics import accuracy_score, f1_score
    if train: model.train()
    else: model.eval()
    preds, trues = [], []
    total, n = 0.0, 0
    with torch.set_grad_enabled(train):
        for X, L, y in dl:
            X, L, y = X.to(DEVICE), L.to(DEVICE), y.to(DEVICE)
            out = model(X, L)
            loss = lossf(out, y)
            if train:
                opt.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(model.parameters(), clip)
                opt.step()
            total += loss.item() * y.size(0)
            n += y.size(0)
            preds += (out.detach().cpu().numpy() > 0.5).astype(int).tolist()
            trues += y.detach().cpu().numpy().astype(int).tolist()
    from sklearn.metrics import accuracy_score, f1_score
    return total / n, accuracy_score(trues, preds), f1_score(trues, preds)

history = []
EPOCHS = 4
for ep in range(1, EPOCHS+1):
    tr_loss, tr_acc, tr_f1 = run_epoch(tr_dl, train=True)
    va_loss, va_acc, va_f1 = run_epoch(te_dl, train=False)
    history.append((ep, tr_loss, va_loss, va_acc, va_f1))
    print({"epoch": ep, "val_acc": round(va_acc,4), "val_f1": round(va_f1,4)})

rnn_metrics = {
    "model":"RNN",
    "acc": float(history[-1][3]),
    "f1":  float(history[-1][4]),
    "auc": float("nan")
}
rnn_metrics

# === DistilBERT fine-tuning ===
# Stops Weights & Biases from asking for an API key
import os
os.environ["WANDB_DISABLED"] = "true"

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset as HFDataset, DatasetDict
from sklearn.metrics import accuracy_score, f1_score
import numpy as np, transformers

print("Transformers version:", transformers.__version__)

# Building HF datasets from already-prepared pandas dataframes: train_df, test_df
hf_train = HFDataset.from_pandas(
    train_df[["text_clean","label"]].rename(columns={"text_clean":"text"})
)
hf_test = HFDataset.from_pandas(
    test_df[["text_clean","label"]].rename(columns={"text_clean":"text"})
)
hds = DatasetDict({"train": hf_train, "test": hf_test})

# Tokenizer + tokenization
tok = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize(batch):
    return tok(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=256
    )

tok_ds = hds.map(tokenize, batched=True).remove_columns(["text"])
tok_ds = tok_ds.rename_column("label", "labels").with_format("torch")

# Metrics callback
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "acc": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds)
    }

# Model
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2
)



args = TrainingArguments(
    output_dir="models/transformer-imdb",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    num_train_epochs=1,
    learning_rate=2e-5,
    logging_steps=100,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tok_ds["train"],
    eval_dataset=tok_ds["test"],
    compute_metrics=compute_metrics
)

trainer.train()

# Final evaluation
tr_metrics = trainer.evaluate()
transformer_metrics = {
    "model": "Transformer",
    "acc": float(tr_metrics.get("eval_acc", np.nan)),
    "f1":  float(tr_metrics.get("eval_f1", np.nan)),
    "auc": float("nan")
}
print(transformer_metrics)

import pandas as pd
import numpy as np

# Combining safely â€” handle missing metrics or bad values
metrics_list = [gbt_metrics, mlp_metrics, transformer_metrics]  # drop rnn if not defined
cmp_df = pd.DataFrame(metrics_list)

# Replacing invalid entries (Ellipsis, NaN, None)
cmp_df = cmp_df.replace({np.nan: 0, None: 0, ...: 0})

# Ensuring numeric sorting works
cmp_df["f1"] = pd.to_numeric(cmp_df["f1"], errors="coerce").fillna(0)

# Sort by f1 descending
cmp_df = cmp_df.sort_values("f1", ascending=False).reset_index(drop=True)

# Saving clean file
cmp_df.to_csv("reports/metrics.csv", index=False)
print("âœ… Saved cleaned comparison to reports/metrics.csv")
display(cmp_df)

import matplotlib.pyplot as plt
plt.figure(figsize=(6,4))
plt.bar(cmp_df["model"], cmp_df["f1"])
plt.title("Model F1 Comparison"); plt.ylabel("F1"); plt.ylim(0,1); plt.show()

import mlflow, numpy as np
from numbers import Number
from pathlib import Path

mlflow.end_run()
mlflow.set_tracking_uri("file:./mlruns")
mlflow.set_experiment("imdb_sentiment")

def log_metrics_safe(metrics: dict):
    clean = {}
    for k, v in metrics.items():
        if v is Ellipsis or v is None:
            continue
        try:
            val = float(v) if isinstance(v, Number) else float(v)
        except Exception:
            continue
        if np.isnan(val) or np.isinf(val):
            continue
        clean[k] = val
    if clean:
        mlflow.log_metrics(clean)

# Map model name -> (metrics_var_name, artifact_path)
runs = [
    ("GBT_TFIDF", "gbt_metrics", "models/gbt_imdb.pkl"),
    ("MLP_TFIDF", "mlp_metrics", "models/mlp_imdb.pkl"),
    ("RNN_BiLSTM", "rnn_metrics", "models/rnn"),
    ("Transformer_DistilBERT", "transformer_metrics", "models/transformer-imdb"),
    ("LogReg_TFIDF", "logr_metrics", "models/logreg_imdb.pkl"),
]

for run_name, var_name, artifact in runs:
    if var_name not in globals():
        continue
    mlflow.end_run()
    mlflow.start_run(run_name=run_name)

    mlflow.log_params({"model_name": run_name})
    log_metrics_safe(globals()[var_name])

    p = Path(artifact)
    if p.exists():
        if p.is_dir():
            mlflow.log_artifacts(str(p))
        else:
            mlflow.log_artifact(str(p))


    if Path("reports/metrics.csv").exists():
        mlflow.log_artifact("reports/metrics.csv")

    mlflow.end_run()
    print(f"âœ… Logged {run_name}")

print("ðŸŽ¯ Done. To view dashboard, run:  !mlflow ui --port 5000")

!pip install -q pyngrok
from pyngrok import ngrok
ngrok.set_auth_token("34U3NbY3Vy0ZNPtUa9hh2vrMKzy_3orUwwiCNLduwESL7UBS8")
print("âœ… Ngrok token configured successfully.")

# Starting MLflow UI in background
!mlflow ui --port 5000 --host 0.0.0.0 &

# Creating the tunnel
from pyngrok import ngrok
public_url = ngrok.connect(5000)
print("âœ… MLflow public URL:", public_url)